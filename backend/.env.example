# Environment variables for the Cogsworth Backend

# Port for the backend server (default is 3001 if not set)
BACKEND_PORT=3001

# --- AI Service Configuration ---
# Replace with the actual URL of your chosen AI service API endpoint
# Example for a local Ollama server: VITE_LLM_API_URL=http://localhost:11434/v1
# Example for OpenAI compatible API: VITE_LLM_API_URL=https://api.example.com/v1
VITE_LLM_API_URL=

# Replace with the actual API key for your chosen AI service
# IMPORTANT: Keep your actual API key in a .env file, NOT committed to Git.
VITE_LLM_API_KEY=

# Optional: Specify the model name if required by the API
# VITE_LLM_MODEL_NAME=llama3